\documentclass[]{article}
\usepackage{amsmath}
%opening
\title{Notes on Machine Learning with R}
\author{Will Steinhardt}

\begin{document}
\newcommand{\code}[1]{\texttt{\detokenize{#1}}}

\maketitle

\begin{abstract}
These notes will summarize the basics of R and some basic machine learning concepts.  They are derived from \emph{Machine Learning with R} by Brett Lantz.
\end{abstract}

\section{Glossary of terms}
\subsection*{Terms relating to R}
\begin{itemize}
	\item \textbf{vector} -- an object holding a collection of \emph{elements} that are all of the same type.
	\item \textbf{factor} -- a data type intended for storing collections of variables belonging to a few different categories.
	\item \textbf{list} -- a data type analogous to vectors where elements need not share the same type.
	\item \textbf{data frame} -- a 2D table of values where rows represent samples and columns represent features, where columns may have different data types.
	\item \textbf{array} -- a n-dimensional means of storing data of a singular data type.
	\item \textbf{package} -- a collection of pre-written functions installed and loaded as a \emph{library}.
\end{itemize}
\subsection*{Terms relating to Machine Learning}
\begin{itemize}
	\item \textbf{nominal feature} -- a feature that is defined by a category (i.e. blood type)
	\item \textbf{test samples} -- data which the neural network was not trained on.
	\item \textbf{test accuracy} -- the rate at which the neural network accurately predicts the category for \emph{test} samples.
	\item \textbf{regularization} -- efforts to stop a neural network from overfitting (capturing fine detail from the training samples).
	\item \textbf{validation set} -- a set of data that is used to determine the current accuracy of the neural network outside of its training data.
	\item \textbf{early stopping} -- a method of regularization by stopping once error plateaus on a validation set.
	\item \textbf{dropout} -- a method of regularization where nodes are randomly turned off during training.  This effectively reduces the size of the neural network, and small networks are less likely to overfit than big ones. 
	\item \textbf{batch learning} -- refers to the process of breaking samples up into batches so that gradient descent is performed over the average over deltas due to all samples in the batch.  This generally allows a larger alpha since the average delta is a better indicator of how to adjust weights to minimize error.
	\item \textbf{activation functions} - functions that provide non-linearity to the action of the weights.  Common examples are \code{relu()}, \code{sigmoid()}, \code{softmax()}, and \code{tanh()}.  These functions and their derivatives should be monotonic, non-linear, and easily computed.  They are what allows \emph{sometimes correlation} in neural networks.
	\item \textbf{bias-variance tradeoff} -- refers to the balance between overfitting and underfitting.
\end{itemize}
\section{Basic syntax and commands in R}
\begin{description}
	\item \code{install.packages("libraryName")} -- install the library.
	\item \code{library(libraryName)} -- load the library into the current project environment. 
	\item \code{<-} -- assign a value to a variable name, (works like = in other languages).
	\item \verb|str(variable)| -- get information about structure of a variable - particularly useful for examining data frames.
	\item \code{c(e1,e2,...,en)} -- combine elements into a vector.
	\item \code{factor(c("e1","e2",...,"en"), levels=c("l1","l2",...,"lm"))} -- make a factor with the $n$ elements and $m$ levels. 
	\item \verb|data.frame(v1,v2,...,vn),stringsAsFactors=FALSE)| -- make a dataframe using the previously defined $n$ vectors, specifying that strings will not be turned into factors.
	\item \verb|dataframeName$columnName| -- access specific column in a data frame.
	\item \verb|read.csv("mydata.csv", stringsAsFactors = FALSE, header = FALSE)| -- load a csv file without a header into a data frame. 
	\item \verb|write.csv(dataframeName, file = "fileName.csv")| -- write a data frame to a csv file.
\end{description}

\section{k Nearest Neighbor (kNN) Classification}
\emph{A machine learning approach that is simple, makes no assumptions about the underlying data, and has a fast training phase.  However, this approach yields no model, is slow at classifying, and requires a large amount of memory.}

\emph{Works when ``you know it when you see it."}

\vspace{0.2cm}


The idea here is to use the Euclidian distance of an object in its space to classify it according to its $k$ nearest neighbors.

In order to use kNN it is important to first rescale the features such that differences in each feature are equally represented.  For this, one typically uses min-max normalization.  Thus for each value of $X$ we determine $X_{scaled}$ as:

$$
X_{scaled} = \frac{X-X_{min}}{X_{max}-X_{min}}
$$

Alternatively, we can also use z-score standardization:

$$
X_{scaled} = \frac{X-\mu}{\sigma}
$$

where $\mu$ is the mean and $\sigma$ is the average.  The value of $X_{scaled}$ here is called the ``z-score'' and describes how many standard deviations the value is from the average.
Nominal features can be ``dummy coded'' to treat them as numerical.  For $n$-category features, we use $(n-1)$ binary indicator variables.  For example, if the categories are red, blue, and green:

\begin{equation}
\text{red} =
\begin{cases}
	\text{1 if $x =$ red}\\
	\text{0 otherwise}
\end{cases}       
\text{blue} = 
\begin{cases}
	\text{1 if $x =$ blue}\\
	\text{0 otherwise}
\end{cases}
\end{equation}

\subsection{Implementing kNN in R}
Implementing kNN has already been accomplished - one package that can be used is the \verb|class| package, which has a number of simple functions for classification.  Building a model for use with the kNN is unnecessary.  We just need to store the data in a format such that we can use it in the \verb|knn()| function.

The arguments for this function go as follows:

\begin{verbatim}
prediction = knn(train = training_data, test = testing_data, 
     cl = class, k = some_integer)
\end{verbatim}

Here the \verb|class| argument is a factor vector with the classes for each row in the training data.

\subsection{Evaluating performance}

It is useful to use \verb|CrossTable| to understand the quality of the prediction.  \verb|CrossTable()| can be found in the \verb|gmodels| package.

We use: 
\begin{verbatim}
CrossTable(x = test_labels, y = test_pred, prop.chisq = FALSE)
\end{verbatim}
Note that if the two vectors \verb|test_labels| and \verb|test_pred| have two possible values, say benign (B) and malignant (M) as in the example cancer study, the CrossTable yields 4 cells indicating the four possible cases (B and predicted as B, B and predicted as M, M and predicted as B, and M and predicted as M).  Thus the better the kNN performs, the fewer the counts in the off-diagonals (where all the regular concerns about over-fitting in machine learning apply).  

\section{Classification with Naive Bayes}

\emph{A technique that offers the probability of an observation belonging to a particular class based on the values of its features.  This approach is handy when numerous attributes need to be considered simultaneously.  However, this approach relies on some assumptions (of equally important and independent features) that may be problematic.}

First, some relevant vocabulary:
\begin{itemize}
\item \textbf{event} -- possible outcome.
\item \textbf{trial} -- single opportunity for even to occur.
\item \textbf{joint probability} -- $P(A \cap B)$ -- probability of both $A$ and $B$.
\item \textbf{conditional probability} -- $P(A|B)$ -- probability of $A$ given $B$.  Note that this describes the fraction of B that is also A, whereas the joint probability describes the fraction of the total which are both A and B.
\item \textbf{Baye's theorem} -- $P(A|B) = \frac{P(B|A)P(A)}{P(B)} = \frac{P(A \cap B)}{P(B)}$
\item \textbf{prior probability} -- estimate of probability neglecting conditional probability and Baye's theorem.  I.e. supposing that 30\% of all animals have 4 legs, and 5\% of all animals are cows, if you are to guess whether an animal is a cow or not with no additional information, your estimate based on prior probability is simply 5\%.
\item \textbf{likelihood} -- seems equivalent to ``conditional probability."
\item \textbf{marginal likelihood} -- seems equivalent to ``prior probability."
\item \textbf{posterior probability} -- conditional probability once we have additional evidence.
\end{itemize}

Bayes' theorem can be used by first generating a frequency table, where rows describe categories and columns represent features.  Each element counts the number of times the feature is observed in the given category.  Dividing each element by the total number of observations of the category yields the relevant conditional probability $P(\text{feature}|\text{category})$, and the resulting table is the likelihood table.

\subsection{Naive Bayes' algorithm}

Suppose there are a number of different Boolean features that can be considered in a classification problem.  Constructing the likelihood table is still straightforward, but computing the conditional probability of a particular classification as the Boolean values are varied can become complex:

$$
P(C_1|F_1 \cap F_2 \cap F_3) = \frac{P(F_1 \cap F_2 \cap F_3|C_1)P(C_1)}{P(F_1 \cap F_2 \cap F_3)}
$$

The difficulty in computing this quantity precisely arises from the need to store all of the probabilities for the intersecting events - i.e. for calculating $P(F_1 \cap F_2 \cap F_3|C_1)$.  Note that this calculation does \emph{not} assume independence between events (and thus the likelihood table representation is not sufficient to allow for this calculation - rather, one would need a Venn diagram or equivalent).  However, the problem can be simplified by assuming class-conditional independence - events are independent for the same class value.  Thus, we assume 
$$
P(F_1 \cap F_2 \cap F_3|C_1) \approx P(F_1|C_1)P(F_2|C_1)P(F_3|C_1)
$$

Thus in the naive Bayes' algorithm Bayes' theorem can be rewritten as:
$$
P(C_1|F_1 \cap F_2 \cap F_3) \approx \frac{P(F_1|C_1)P(F_2|C_1)P(F_3|C_1)P(C_1)}{P(F_1)P(F_2)P(F_3)}
$$

Note that for different classifications $C_1$, $C_2$, ..., $C_n$ the denominator will always be the same.  Then only the numerators need to be calculated, and probabilities can be calculated by dividing each by the sum of the numerators, denoted $Z$, like the partition function.  This can all be written succinctly as:
$$
P(C_L|F1,...,F_n) \approx \frac{1}{Z}P(C_L)\prod_{i=1}^{n}P(F_i|C_L)
$$

\subsection{The Laplace Estimator}

A possible issue with the naive Bayes' algorithm is the case where an event never occurs for one or more levels of the class. This then leads to a prediction of zero-probability due to the presence of the 0 in the product. The solution is to use the Laplace estimator, which just adds a small number (usually 1) to every count in the frequency table.

\end{document}